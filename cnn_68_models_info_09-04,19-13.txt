data_set_size:34117
accuracy of each sign:
sign 0, accuracy 0.986111 (71 / 72)
sign 1, accuracy 0.981481 (106 / 108)
sign 2, accuracy 0.971014 (67 / 69)
sign 3, accuracy 0.870968 (54 / 62)
sign 4, accuracy 0.961039 (74 / 77)
sign 5, accuracy 0.917647 (78 / 85)
sign 6, accuracy 0.940594 (95 / 101)
sign 7, accuracy 0.000000 (0 / 63)
sign 8, accuracy 0.980392 (50 / 51)
sign 9, accuracy 0.974359 (76 / 78)
sign 10, accuracy 0.989362 (93 / 94)
sign 11, accuracy 0.987013 (76 / 77)
sign 12, accuracy 0.987179 (77 / 78)
sign 13, accuracy 0.977011 (85 / 87)
sign 14, accuracy 0.936508 (59 / 63)
sign 15, accuracy 0.934211 (71 / 76)
sign 16, accuracy 0.983607 (60 / 61)
sign 17, accuracy 0.951613 (59 / 62)
sign 18, accuracy 0.985507 (68 / 69)
sign 19, accuracy 0.969697 (32 / 33)
sign 20, accuracy 0.978261 (45 / 46)
sign 21, accuracy 0.969231 (63 / 65)
sign 22, accuracy 0.984127 (62 / 63)
sign 23, accuracy 0.930556 (67 / 72)
sign 24, accuracy 0.000000 (0 / 63)
sign 25, accuracy 0.954128 (104 / 109)
sign 26, accuracy 0.984615 (64 / 65)
sign 27, accuracy 0.000000 (0 / 67)
sign 28, accuracy 0.000000 (0 / 32)
sign 29, accuracy 0.000000 (0 / 40)
sign 30, accuracy 1.000000 (47 / 47)
sign 31, accuracy 0.941860 (81 / 86)
sign 32, accuracy 1.000000 (78 / 78)
sign 34, accuracy 0.984127 (124 / 126)
sign 36, accuracy 0.982143 (55 / 56)
sign 37, accuracy 1.000000 (22 / 22)
sign 38, accuracy 0.932432 (69 / 74)
sign 40, accuracy 0.000000 (0 / 2)
sign 41, accuracy 1.000000 (60 / 60)
sign 42, accuracy 0.957447 (45 / 47)
sign 44, accuracy 0.983607 (60 / 61)
sign 45, accuracy 0.948276 (55 / 58)
sign 46, accuracy 0.000000 (0 / 39)
sign 49, accuracy 0.919355 (57 / 62)
sign 50, accuracy 0.918033 (56 / 61)
sign 51, accuracy 1.000000 (30 / 30)
sign 52, accuracy 0.000000 (0 / 44)
sign 53, accuracy 0.923077 (24 / 26)
sign 54, accuracy 0.000000 (0 / 24)
sign 55, accuracy 0.000000 (0 / 62)
sign 56, accuracy 0.952381 (60 / 63)
sign 57, accuracy 0.000000 (0 / 40)
sign 58, accuracy 1.000000 (64 / 64)
sign 59, accuracy 1.000000 (53 / 53)
sign 60, accuracy 0.980769 (51 / 52)
sign 61, accuracy 1.000000 (24 / 24)
sign 62, accuracy 0.962963 (26 / 27)
sign 63, accuracy 0.000000 (0 / 44)
sign 64, accuracy 0.000000 (0 / 44)
sign 65, accuracy 0.000000 (0 / 25)
sign 66, accuracy 0.988636 (87 / 88)
sign 67, accuracy 1.000000 (61 / 61)
sign 68, accuracy 0.961538 (50 / 52)
overall accuracy: 0.81662
loss: 3.418395
Epoch: 700
CNN(
  (conv1): Sequential(
    (0): Conv1d(14, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01)
    (3): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))
    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): LeakyReLU(negative_slope=0.01)
    (1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv3): Sequential(
    (0): LeakyReLU(negative_slope=0.01)
    (1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (out1): Sequential(
    (0): LeakyReLU(negative_slope=0.01)
    (1): Dropout(p=0.5)
    (2): Linear(in_features=768, out_features=512, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Dropout(p=0.5)
    (5): Linear(in_features=512, out_features=512, bias=True)
    (6): Tanh()
    (7): Dropout(p=0.5)
    (8): Linear(in_features=512, out_features=69, bias=True)
    (9): Softmax()
  )
)