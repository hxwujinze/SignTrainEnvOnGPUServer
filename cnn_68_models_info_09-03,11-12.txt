data_set_size:34117
accuracy of each sign:
sign 0, accuracy 0.977011 (85 / 87)
sign 1, accuracy 0.990291 (102 / 103)
sign 2, accuracy 0.964286 (54 / 56)
sign 3, accuracy 0.000000 (0 / 67)
sign 4, accuracy 0.974684 (77 / 79)
sign 5, accuracy 0.959459 (71 / 74)
sign 6, accuracy 0.970588 (99 / 102)
sign 7, accuracy 0.921875 (59 / 64)
sign 8, accuracy 0.980392 (50 / 51)
sign 9, accuracy 0.985507 (68 / 69)
sign 10, accuracy 1.000000 (85 / 85)
sign 11, accuracy 1.000000 (76 / 76)
sign 12, accuracy 1.000000 (83 / 83)
sign 13, accuracy 0.960526 (73 / 76)
sign 14, accuracy 0.981481 (53 / 54)
sign 15, accuracy 0.988764 (88 / 89)
sign 16, accuracy 0.979167 (47 / 48)
sign 17, accuracy 0.957746 (68 / 71)
sign 18, accuracy 0.986111 (71 / 72)
sign 19, accuracy 1.000000 (33 / 33)
sign 20, accuracy 0.000000 (0 / 53)
sign 21, accuracy 0.927273 (51 / 55)
sign 22, accuracy 0.984375 (63 / 64)
sign 23, accuracy 0.927536 (64 / 69)
sign 24, accuracy 0.985714 (69 / 70)
sign 25, accuracy 0.978723 (138 / 141)
sign 26, accuracy 1.000000 (72 / 72)
sign 27, accuracy 0.967213 (59 / 61)
sign 28, accuracy 0.000000 (0 / 27)
sign 29, accuracy 0.977778 (44 / 45)
sign 30, accuracy 0.902439 (37 / 41)
sign 31, accuracy 1.000000 (72 / 72)
sign 32, accuracy 1.000000 (78 / 78)
sign 34, accuracy 0.960938 (123 / 128)
sign 36, accuracy 1.000000 (55 / 55)
sign 37, accuracy 1.000000 (40 / 40)
sign 38, accuracy 1.000000 (82 / 82)
sign 40, accuracy 0.000000 (0 / 2)
sign 41, accuracy 1.000000 (53 / 53)
sign 42, accuracy 0.981132 (52 / 53)
sign 44, accuracy 1.000000 (37 / 37)
sign 45, accuracy 0.952381 (40 / 42)
sign 46, accuracy 1.000000 (39 / 39)
sign 49, accuracy 0.972222 (70 / 72)
sign 50, accuracy 0.950820 (58 / 61)
sign 51, accuracy 0.000000 (0 / 26)
sign 52, accuracy 0.000000 (0 / 44)
sign 53, accuracy 0.971429 (34 / 35)
sign 54, accuracy 0.000000 (0 / 19)
sign 55, accuracy 1.000000 (67 / 67)
sign 56, accuracy 0.968750 (62 / 64)
sign 57, accuracy 1.000000 (39 / 39)
sign 58, accuracy 1.000000 (75 / 75)
sign 59, accuracy 1.000000 (58 / 58)
sign 60, accuracy 0.000000 (0 / 53)
sign 61, accuracy 0.000000 (0 / 21)
sign 62, accuracy 0.942857 (33 / 35)
sign 63, accuracy 0.000000 (0 / 33)
sign 64, accuracy 1.000000 (46 / 46)
sign 65, accuracy 1.000000 (38 / 38)
sign 66, accuracy 1.000000 (69 / 69)
sign 67, accuracy 1.000000 (65 / 65)
sign 68, accuracy 0.961538 (50 / 52)
overall accuracy: 0.89024
loss: 3.361489
Epoch: 700
CNN(
  (conv1): Sequential(
    (0): Conv1d(14, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01)
    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): MaxPool1d(kernel_size=4, stride=3, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): LeakyReLU(negative_slope=0.01)
    (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv3): Sequential(
    (0): LeakyReLU(negative_slope=0.01)
    (1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)
  )
  (out1): Sequential(
    (0): LeakyReLU(negative_slope=0.01)
    (1): Dropout(p=0.5)
    (2): Linear(in_features=1152, out_features=512, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Dropout(p=0.5)
    (5): Linear(in_features=512, out_features=256, bias=True)
    (6): Tanh()
    (7): Dropout(p=0.5)
    (8): Linear(in_features=256, out_features=69, bias=True)
    (9): Softmax()
  )
)